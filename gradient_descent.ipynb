{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Градиентный спуск\n",
    "\n",
    "В этом задании нам предстоит реализовать классический алгоритм градиентного спуска для обучения модели логистической регрессии.\n",
    "\n",
    "Алгоритм выполнения этого задания следующий:\n",
    "\n",
    "* На основе посчитанных в первом задании частных производных, напишем функцию подсчета градиента бинарной кросс-энтропии по параметрам модели\n",
    "\n",
    "* Напишем функцию обновления весов по посчитанным градиентам\n",
    "\n",
    "* Напишем функцию тренировки модели\n",
    "\n",
    "Замечание:\n",
    "Тренировка модели проводится в несколько циклов, в рамках каждого из которых мы обновим веса модели, основываясь на предсказании для **каждого** объекта из датасета. Такие циклы называются *эпохами*. То есть одна эпоха - это набор обновлений весов, реализованный согласно посчитанным для каждого объекта из датасета ошибкам модели.\n",
    "\n",
    "Вам необходимо реализовать обучение модели в несколько эпох. Их количество задается параметром функции. В рамках каждой эпохи необходимо пройти циклом по всем объектам обучающей выборки и обновить веса модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "# Функция подсчета градиента\n",
    "def gradient(y_true: int, y_pred: float, x: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    y_true - истинное значение ответа для объекта x\n",
    "    y_pred - значение степени принадлежности объекта x классу 1, предсказанное нашей моделью\n",
    "    x - вектор признакового описания данного объекта\n",
    "\n",
    "    На выходе ожидается получить вектор частных производных H по параметрам модели, предсказавшей значение y_pred\n",
    "    Обратите внимание, что размерность этого градиента должна получиться на единицу больше размерности x засчет своободного коэффициента a0\n",
    "    \"\"\"\n",
    "    grad = x*((1-y_true)*y_pred - y_true*(1-y_pred))\n",
    "    return grad\n",
    "\n",
    "\n",
    "# Функция обновления весов\n",
    "def update(alpha: np.array, gradient: np.array, lr: float):\n",
    "    \"\"\"\n",
    "    alpha: текущее приближения вектора параметров модели\n",
    "    gradient: посчитанный градиент по параметрам модели\n",
    "    lr: learning rate, множитель перед градиентом в формуле обновления параметров\n",
    "    \"\"\"\n",
    "    alpha_new = alpha - lr*gradient\n",
    "    return alpha_new\n",
    "\n",
    "\n",
    "# функция тренировки модели\n",
    "def train(\n",
    "    alpha0: np.array, x_train: np.array, y_train: np.array, lr: float, num_epoch: int\n",
    "):\n",
    "    \"\"\"\n",
    "    alpha0 - начальное приближение параметров модели\n",
    "    x_train - матрица объект-признак обучающей выборки\n",
    "    y_train - верные ответы для обучающей выборки\n",
    "    lr - learning rate, множитель перед градиентом в формуле обновления параметров\n",
    "    num_epoch - количество эпох обучения, то есть полных 'проходов' через весь датасет\n",
    "    \"\"\"\n",
    "    alpha = alpha0.copy()\n",
    "    for epo in range(num_epoch):\n",
    "        for i, x in enumerate(x_train):\n",
    "            x = np.hstack([x,np.array([1])]) # Добавил в конец строки признаков объекта единицу чтобы (она домнажается на свободный член альфа0)\n",
    "            y_pred = 1/(1+np.exp(alpha*x))\n",
    "            grad = gradient(y_train[i],y_pred,x)\n",
    "            alpha = update(alpha,grad,lr)\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Замечания:\n",
    "\n",
    "1. В случае, если у Вас возникли сложности с выполнением первого задания и, как следствие, у Вас не выходит сделать это, мы рекомендуем подробно ознакомиться с главой **Производные $\\frac{\\partial H}{\\partial \\omega_i}$** нашей [лекции](https://colab.research.google.com/drive/1xjX_YnXcRr8HSiYLByMHxEIAADqs7QES?usp=sharing).\n",
    "\n",
    "2. Обращайте внимание на названия и порядок аргументов в сдаваемых на проверку функциях - они должны совпадать с тем, что указано в шаблоне кода.\n",
    "\n",
    "3. Обратите внимание, что матрица объект-признак в описании параметров функций обозначает переменную типа numpy.array(), каждый элемент которой - объект типа numpy.array() - вектор признаков соответствующего объекта.\n",
    "\n",
    "4. Считайте, что свободный коэффициент a0 находится **в конце** списка alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сигмоид $$σ(x) = \\frac{1}{1 + e^{-x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задаче бинарной классификации в качестве loss-функции обычно используется бинарная кросс-энтропия (Binary Cross-Entropy, BCE). Эта функция зависит от настоящих меток $y$ и предсказаний алгоритма $a(x) = p$, и выглядит следующим образом:\n",
    "\n",
    "$$H(p, y) = - (y \\cdot ln(p) +(1 - y) \\cdot ln(1-p))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из математического анализа известно, что если мы имеем функцию $F = F(x_1 ... x_n)$, то вектор [частных производных](https://colab.research.google.com/drive/1BODfwl4F3c7h0CE-t88zavkZWyNe8n5G#scrollTo=aGVowH1ufYP_) этой функции (называемый градиентом), $$\\vec{grad(F)} = \\vec{∇F} = (\\frac{\\partial F(x_1)}{\\partial x_1} ... \\frac{\\partial F(x_n)}{\\partial x_n})$$ \n",
    "направлен в сторону **наискорейшего роста функции**. Вектор же **антиградиента**, соответственно, направлен в направлении наискорейшего убывания функции. Если мы хотим *шаг за шагом* приближаться к минимуму функции, мы должны каждый раз делать небольшой шажок в направлении антиградиента. Математически эту идею можно записать в виде следующей формулы: $$\\vec{x_{n+1}} = \\vec{x_n} - λ\\vec{∇F(x)}$$ Где $λ$ характеризует размер нашего шага.\n",
    "\n",
    "Применив эту идею для поиска минимума Loss-функции по параметрам нашей модели $\\vec{w}$, мы найдем оптимальный (или почти оптимальный) вектор параметров модели, который и будем использовать при классификации. Запомним эту идею!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Производные $\\frac{\\partial H}{\\partial \\omega_i}$\n",
    "\n",
    "В рамках алгоритма градиентного спуска нам необходимо посчитать [производные](https://colab.research.google.com/drive/1BODfwl4F3c7h0CE-t88zavkZWyNe8n5G#scrollTo=1m66zhVnUv9w) функции бинарной кросс-энтропии по параметрам модели, чтобы делать шаги градиентного спуска.\n",
    "\n",
    "Покажем, как можно посчитать эту производную.\n",
    "\n",
    "Рассмотрим случай, когда нам необходимо посчитать производные всего для одного примера. То есть мы имеем единственный объект $x$ с истинной меткой $y$ и гипотезой нашего алгоритма $p$. Тогда $$H(p, y) = - (y \\cdot ln(p) +(1 - y) \\cdot ln(1-p))$$\n",
    "\n",
    "Причем $p = σ(ω_1 \\cdot x_1 + ... + ω_{n} \\cdot x_n + ω_0 ⋅ 1)$\n",
    "\n",
    "Вспомним простое правило из математического анализа о вычисленнии [производной сложной функции](https://colab.research.google.com/drive/1BODfwl4F3c7h0CE-t88zavkZWyNe8n5G#scrollTo=qny-9SFZUibG):\n",
    "\n",
    "$$\\frac{∂ H}{∂ ω_i} = \\frac{∂ H}{∂ p} \\cdot \\frac{∂ p}{∂ ω_i}$$\n",
    "Посчитаем первое и второе слагаемое по отдельности.\n",
    "- $\\frac{∂ H}{∂ p} = -y ⋅ \\frac{1}{p} + (1-y) \\cdot \\frac{1}{1 - p}$\n",
    "\n",
    "Вспомним одно из основных свойств сигмоидальной функции (его доказательство - полезное упражнение, рекомендуется выполнить его самостоятельно):\n",
    "$σ' = σ \\cdot (1 - σ)$\n",
    "- $\\frac{∂ p}{∂ ω_i} = p⋅(1 - p)⋅x_i$, если $i \\neq 0$\n",
    "- $\\frac{∂ p}{∂ ω_0} = p⋅(1 - p)$\n",
    "\n",
    "Введём обозначение $x_0 ≡ 1$\n",
    "\n",
    "Тогда оба равенства можно записать $\\frac{∂ p}{∂ ω_i} = p⋅(1 - p)⋅x_i$\n",
    "\n",
    "Перемножим полученные результаты:\n",
    "\n",
    "$$\\frac{∂ H}{∂ ω_i} = p⋅(1 - p)⋅x_i ⋅ (-y ⋅ \\frac{1}{p} + (1-y) \\cdot \\frac{1}{1 - p}) = x_i((1 - y)⋅p - y⋅(1-p))$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
